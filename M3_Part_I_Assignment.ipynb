{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanfra21/nlp-yu/blob/main/M3_Part_I_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment 3 Naïve Bayes and Sentiment Classification and Logistic Regression\n",
        "Instructions\n",
        "* Read the following Chapter 4: Naive Bayes and Sentiment Classification. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "* Read the following Chapter 5: Logistic Regression. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "\n",
        "Summary\n",
        "Classification is one of the most important tasks of NLP and in machine learning. In NLP it often means the task of text categorization for both sentiment analysis, spam detection, and topic modeling. Naïve Bayes is often one of the first classification algorithms defined in NLP.  The intuition behind a classifier is lies at the underlying probability inferred by the Bayesian Inference, which uses Baye’s rule and conditional probabilities.\n",
        "\n",
        "Here’s a reminder on Baye’s Rule:\n",
        "P(y)=P(x)P(x)/(P(y))\n",
        "\n",
        "We are saying “what is the probability of x given y”. Naïve Bayes is a generative model because there is an input that helps the model determine what the output could be. Said differently, “to train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it.” [6]\n",
        "\n",
        "So in the case of Naïve Bayes, we say given some word, what should be the class of the current word we are assessing? Contrastingly, discriminative models such as logistic regression, learn from features provided to the algorithm and then determine or predict what the class is. [7]\n",
        "\n",
        "\n",
        "With Naïve Bayes, the assumption is that the probabilities are independent. We often call the Naïve Bayes classifier the bag-of-words approach. That’s because we are essentially throwing in the collection of words into a ‘bag’, selecting a word at random, and then calculating their frequency to use in the Bayesian Inference. Thus, context – the position of words -- is ignored and despite this, it turns out that the Naïve Bayes approach can be accurate and effective at determining whether an email is spam for example.\n",
        "\n",
        "Back to bag of words. With bag of words, we assume that the position of the words are not relevant -- that dependency or context in the word phrase or sentence doesn’t matter. Relatedly, the naive Bayes assumption implies that the conditional probabilities are independent -- a rather strange assumption to make for words in a sentence! The equation for the naive Bayes classifier is outlined below:\n",
        "\n",
        "You can use Naive Bayes by creating an index of words and walking through every word position in a test or corpus.\n"
      ],
      "metadata": {
        "id": "liqKR9Vk9RSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.\n",
        "\n",
        "For this Assignment, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:   https://spamassassin.apache.org/old/publiccorpus/\n",
        "\n",
        "You may work alone or in a group on this project.  You're welcome to use any tools or approach that you like.  Due before our next meetup. Starter code provided below.\n",
        "\n",
        "Test example is provided at the end."
      ],
      "metadata": {
        "id": "CIBB2IVT92Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries you may wish to use"
      ],
      "metadata": {
        "id": "c8sZQL-a-cHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import makedirs, path, remove, rename, rmdir\n",
        "from tarfile import open as open_tar\n",
        "from shutil import rmtree\n",
        "from urllib import request, parse\n",
        "from glob import glob\n",
        "from os import path\n",
        "from re import sub\n",
        "from email import message_from_file\n",
        "from glob import glob\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import gc"
      ],
      "metadata": {
        "id": "NHiCf9fi9103"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download corpus using the following functions\n",
        "\n",
        "Note: you may need to mount your drive on google then run this location. See previous exercises."
      ],
      "metadata": {
        "id": "uObO057u-Rne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C4fIGS9-8wce"
      },
      "outputs": [],
      "source": [
        "def download_corpus(dataset_dir: str = 'data'):\n",
        "    base_url = 'https://spamassassin.apache.org'\n",
        "    corpus_path = 'old/publiccorpus'\n",
        "    files = {\n",
        "        '20021010_easy_ham.tar.bz2': 'ham',\n",
        "        '20021010_hard_ham.tar.bz2': 'ham',\n",
        "        '20021010_spam.tar.bz2': 'spam',\n",
        "        '20030228_easy_ham.tar.bz2': 'ham',\n",
        "        '20030228_easy_ham_2.tar.bz2': 'ham',\n",
        "        '20030228_hard_ham.tar.bz2': 'ham',\n",
        "        '20030228_spam.tar.bz2': 'spam',\n",
        "        '20030228_spam_2.tar.bz2': 'spam',\n",
        "        '20050311_spam_2.tar.bz2': 'spam' }\n",
        "\n",
        "    #creates the folders: downloads, ham and spam\n",
        "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
        "    ham_dir = path.join(dataset_dir, 'ham')\n",
        "    spam_dir = path.join(dataset_dir, 'spam')\n",
        "\n",
        "    makedirs(downloads_dir, exist_ok=True)\n",
        "    makedirs(ham_dir, exist_ok=True)\n",
        "    makedirs(spam_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    for file, spam_or_ham in files.items():\n",
        "        # download files from URL of each specific .bz2 file\n",
        "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
        "        tar_filename = path.join(downloads_dir, file)\n",
        "        request.urlretrieve(url, tar_filename)\n",
        "\n",
        "        #list e-mails in the compressed .bz2 file\n",
        "        emails = []\n",
        "        with open_tar(tar_filename) as tar:\n",
        "            tar.extractall(path=downloads_dir)\n",
        "            for tarinfo in tar:\n",
        "                if len(tarinfo.name.split('/')) > 1:\n",
        "                    emails.append(tarinfo.name)\n",
        "\n",
        "        # move e-mails to ham or spam directory\n",
        "        for email in emails:\n",
        "            directory, filename = email.split('/')\n",
        "            directory = path.join(downloads_dir, directory)\n",
        "\n",
        "            if not path.exists(path.join(dataset_dir, spam_or_ham, filename)):\n",
        "                rename(path.join(directory, filename),\n",
        "                   path.join(dataset_dir, spam_or_ham, filename))\n",
        "\n",
        "        rmtree(directory)\n",
        "\n",
        "download_corpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n"
      ],
      "metadata": {
        "id": "MUmHvbCn-o3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n",
        "ham_dir = path.join('data', 'ham')\n",
        "spam_dir = path.join('data', 'spam')\n",
        "\n",
        "print('Number of Non-Spam E-mails:', len(glob(f'{ham_dir}/*')))\n",
        "print('\\nNumber of Spam E-mails:', len(glob(f'{spam_dir}/*')))"
      ],
      "metadata": {
        "id": "Cx-Blo33-oM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57764d7b-c896-40a2-d218-7ff8f75e62fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Non-Spam E-mails: 6952\n",
            "\n",
            "Number of Spam E-mails: 2399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier"
      ],
      "metadata": {
        "id": "v3fSuJ0G_jNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read and store emails"
      ],
      "metadata": {
        "id": "-wR9L36twwGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create two functions which will help us to read and store the emails.\n",
        "\n",
        "Emails can be multipart, as such, we need to create a function that will help us concatenate its parts in case they are, or if they are not multipart, just return the content of the email.\n",
        "\n",
        "Then the second function, will read through the directory and return the emails and their subject as a string"
      ],
      "metadata": {
        "id": "hoJGRm33xRlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_payload_recursive(msg):\n",
        "    if msg.is_multipart():\n",
        "        parts = [get_payload_recursive(part) for part in msg.get_payload()]\n",
        "        return ''.join(parts)\n",
        "    else:\n",
        "        return msg.get_payload()\n",
        "\n",
        "def read_emails(directory):\n",
        "    email_files = glob(f'{directory}/*')\n",
        "    emails = []\n",
        "    for email_file in email_files:\n",
        "        with open(email_file, 'r', encoding='latin1') as file:\n",
        "            msg = message_from_file(file)\n",
        "            subject = msg['subject'] if msg['subject'] is not None else ''  # Handle potential None values for subject\n",
        "            payload = get_payload_recursive(msg) if msg.get_payload() is not None else ''  # Handle potential None values for content\n",
        "            emails.append(subject + '\\n' + payload)\n",
        "    return emails\n",
        "\n",
        "# Read emails from each directory\n",
        "ham_emails = read_emails(ham_dir)\n",
        "spam_emails = read_emails(spam_dir)"
      ],
      "metadata": {
        "id": "xsV6JwD5v7Yf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have two lists containing the emails as strings, let's see how it looks like:"
      ],
      "metadata": {
        "id": "4frELgntHosY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spam_emails[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4xHEn7K1GBQ",
        "outputId": "0b36cc23-1da5-440f-857b-426e1135ff03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FORTUNE 500 COMPANY HIRING, AT HOME REPS.\n",
            "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
            "growing at a tremendous rate.  We are looking for individuals who\n",
            "want to work from home.\n",
            "\n",
            "This is an opportunity to make an excellent income.  No experience\n",
            "is required.  We will train you.\n",
            "\n",
            "So if you are looking to be employed from home with a career that has\n",
            "vast opportunities, then go:\n",
            "\n",
            "http://www.basetel.com/wealthnow\n",
            "\n",
            "We are looking for energetic and self motivated people.  If that is you\n",
            "than click on the link and fill out the form, and one of our\n",
            "employement specialist will contact you.\n",
            "\n",
            "To be removed from our link simple go to:\n",
            "\n",
            "http://www.basetel.com/remove.html\n",
            "\n",
            "\n",
            "7749doNL1-136DfsE5701lGxl2-486pAKM7127JwoR4-054PCfq9499xMtW0-594hucS91l66\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Pre-Processing"
      ],
      "metadata": {
        "id": "IgBBqiRSdLS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will create a function for our text pre-processing pipeline, which will do the following:\n",
        "\n",
        "1. Tokenize the emails.\n",
        "2. Remove all punctuation within the emails.\n",
        "3. Lowercase the emails.\n",
        "4. Remove stop words.\n",
        "5. Perform lemmatization on the emails.\n",
        "6. Remove all tokens with less than 1 character."
      ],
      "metadata": {
        "id": "N12x2PtUdlJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr3REXupzPt3",
        "outputId": "202fbdac-fb33-4202-b434-c2b1588cea0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define english stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Define function to remove punctuation\n",
        "translator = str.maketrans('', '', punctuation)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = [token.translate(translator) for token in text]\n",
        "\n",
        "    # Lowercase\n",
        "    text = [token.lower() for token in text]\n",
        "\n",
        "    # Remove stop words\n",
        "    text = [token for token in text if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text]\n",
        "\n",
        "    # Remove tokens with less than 1 character\n",
        "    text = [token for token in text if len(token) >= 1]\n",
        "\n",
        "    return text\n",
        "\n",
        "preprocessed_ham = []\n",
        "preprocessed_spam = []\n",
        "\n",
        "preprocessed_ham = [preprocess_text(email) for email in ham_emails]\n",
        "preprocessed_spam = [preprocess_text(email) for email in spam_emails]"
      ],
      "metadata": {
        "id": "XcWIukt2xAe1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see a before vs after example:"
      ],
      "metadata": {
        "id": "DOXixFttII-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ham_emails[129])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6i-XP4t9tcC",
        "outputId": "7fe004f3-f164-4021-cc77-53ac1ae1fffe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re: Java is for kiddies\n",
            "JoeBar wrote:\n",
            ">C is more reliable than Java??\n",
            "\n",
            "Depends who writes it.  One guy will write a bug every 5 lines,\n",
            "another every 5000 lines.  Put them both on a project and that will\n",
            "average out to a bug every 4.995 lines.\n",
            "<observation type=trivial>(Irrespective of language.  Pick the one\n",
            "that best suits what you're trying to do.)</observation>\n",
            "\n",
            "R\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_ham[129])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quYGbJYUCsS0",
        "outputId": "48498473-27e6-4d41-c636-90fe5dc31d56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['java', 'kiddy', 'joebar', 'wrote', 'c', 'reliable', 'java', 'depends', 'writes', 'one', 'guy', 'write', 'bug', 'every', '5', 'line', 'another', 'every', '5000', 'line', 'put', 'project', 'average', 'bug', 'every', '4995', 'line', 'observation', 'typetrivial', 'irrespective', 'language', 'pick', 'one', 'best', 'suit', 'trying', 'observation', 'r']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test and train split and Term-Document Matrix"
      ],
      "metadata": {
        "id": "EwoAE58ydXUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will proceed to create the term-document matrix with the already preprocessed emails."
      ],
      "metadata": {
        "id": "2XJa5Vladl8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the CountVectorizer from scikit-learn\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "def create_train_test_sets(preprocessed_ham, preprocessed_spam):\n",
        "    # Combine the emails into a single list of emails\n",
        "    all_emails = preprocessed_ham + preprocessed_spam\n",
        "\n",
        "    # Create labels for the emails (1 for ham, 0 for spam)\n",
        "    labels = [1] * len(preprocessed_ham) + [0] * len(preprocessed_spam)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(all_emails, labels, test_size=0.2, random_state=2)\n",
        "\n",
        "    # Fit the vectorizer to the training emails and transform them into a term-document matrix\n",
        "    X_train = vectorizer.fit_transform([' '.join(email) for email in X_train])\n",
        "\n",
        "    # Transform the testing emails into a term-document matrix\n",
        "    X_test = vectorizer.transform([' '.join(email) for email in X_test])\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = create_train_test_sets(preprocessed_ham, preprocessed_spam)"
      ],
      "metadata": {
        "id": "VWewhXIvdUry"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Model"
      ],
      "metadata": {
        "id": "8r0p74bYdfDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create a Multinomial Naive Bayes model for spam detection and fit it to the training data, then we make predictions on the test data"
      ],
      "metadata": {
        "id": "SoY7o2iTdmhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Create a Multinomial Naive Bayes model\n",
        "spam_detection_mnb = MultinomialNB()\n",
        "\n",
        "# Fit the model to the training data\n",
        "spam_detection_mnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = spam_detection_mnb.predict(X_test)"
      ],
      "metadata": {
        "id": "A0gtWz4cdJqa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the accuracy of our model"
      ],
      "metadata": {
        "id": "ZaC4LwrQOusz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wjpiKMMEcQQ",
        "outputId": "afc66950-a34d-4cc5-d423-59804bab806f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9796900053447354"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the model with the test email. For that we need to preprocess it and vectorize it. Which is the format that our model requires.\n",
        "\n",
        "By using  ``` predict_proba ``` we can obtain the probability that the model estimates of an email to be ham or spam.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J5WtArb0_IMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_email = \"\"\"\n",
        "Subject: Get Rich Quick!\n",
        "\n",
        "Dear Friend,\n",
        "\n",
        "Congratulations! You've been selected to participate in an exclusive opportunity to make thousands of dollars from the comfort of your own home. Our revolutionary system guarantees quick and easy cash with minimal effort.\n",
        "\n",
        "No more struggling to pay bills or worrying about financial security. With our proven method, you can start earning massive amounts of money in no time.\n",
        "\n",
        "Here's what some of our satisfied customers have to say:\n",
        "- \"I was skeptical at first, but I'm now living my dream life thanks to this incredible system!\" - John S.\n",
        "- \"I never thought making money online could be this simple. It's changed my life!\" - Sarah L.\n",
        "\n",
        "Don't miss out on this limited-time offer. Act now to secure your spot and start enjoying a life of financial freedom.\n",
        "\n",
        "Click the link below to get started:\n",
        "www.getrichquick.com\n",
        "\n",
        "Remember, this opportunity is exclusive and won't last long. Take control of your financial future today!\n",
        "\n",
        "Best regards,\n",
        "The Get Rich Quick Team\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_lvIjkRW_O8e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_email(email):\n",
        "    # Preprocess the custom email\n",
        "    preprocessed_custom_email = preprocess_text(email)\n",
        "\n",
        "    # Convert the preprocessed email into a term-document matrix\n",
        "    X_custom = vectorizer.transform([' '.join(preprocessed_custom_email)])\n",
        "\n",
        "    # Predict the sentiment of the custom email\n",
        "    predicted_sentiment = spam_detection_mnb.predict(X_custom)[0]\n",
        "\n",
        "    if predicted_sentiment == 1:\n",
        "        print(\"This email is predicted to be ham:\")\n",
        "    else:\n",
        "        print(\"This email is predicted to be spam:\")\n",
        "\n",
        "    print(\"- The probability of this email to be ham is:\", round((spam_detection_mnb.predict_proba(X_custom)[0][1])*100,2),\"%\")\n",
        "    print(\"- The probability of this email to be spam is:\", round((spam_detection_mnb.predict_proba(X_custom)[0][0])*100,2),\"%\")\n",
        "\n",
        "test_email(spam_email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93hfjALJEjy-",
        "outputId": "f6aad263-3a07-4fb4-ff9d-2344126ad853"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This email is predicted to be spam:\n",
            "- The probability of this email to be ham is: 0.0 %\n",
            "- The probability of this email to be spam is: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, it's correctly classified"
      ],
      "metadata": {
        "id": "9e-Tl3qdPIeF"
      }
    }
  ]
}