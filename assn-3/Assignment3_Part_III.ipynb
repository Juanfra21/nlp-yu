{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanfra21/nlp-yu/blob/main/assn-3/Assignment3_Part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ7DMQe6ynxH"
      },
      "source": [
        "##Part III:  Machine Learning and Deep Neural Networks with NLP\n",
        "\n",
        "Next we will move to Machine Learning Models and the Introduction of Deep Neural networks for NLP.\n",
        "\n",
        "In this section, we will cover:\n",
        "\n",
        "\n",
        "1.   Refresher on Machine Learning and Shallow Learning Approach\n",
        "2.   Introduction to Neural Networks and Deep Learning\n",
        "3.   Sequence Models with Neural Networks\n",
        "\n",
        "## Setup\n",
        "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n",
        "\n",
        "It will be look like following:\n",
        "```\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "'Some coding activity for you to complete'\n",
        "### END CODE HERE ###\n",
        "\n",
        "```\n",
        "Please be sure to fill these code snippets out as you turn in your assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2gM5VJY33wG"
      },
      "source": [
        "### 3.1 Machine Learning for NLP\n",
        "Recall that we can use our techniques to create predictive algorithms and solve common NLP tasks/goals such as sentiment analysis, text summarization, question-answering, etc. These tasks, you will find, are greatly improved with Deep Learning and Neural Networks.\n",
        "\n",
        "\n",
        "![Artificial Intelligence](https://drive.google.com/uc?export=view&id=1cMW6E4PiVPvxvlfS7IxrBNkv2byAelXy)\n",
        "\n",
        "\n",
        "Before move towards understanding the NN used for NLP, let's briefly refresh our understanding of Machine Learning, or shallow learning techniques.\n",
        "\n",
        "There are several fundamental steps to any Machine Learning algorithm. Typically, they follow these steps below.\n",
        "\n",
        "![basic ML](https://drive.google.com/uc?export=view&id=1cNhv3qDj_j8Mvga274azmRYJ0LzC2bxx)\n",
        "\n",
        "One of the most common use cases is classification of data. We use a supervised machine learning model where some body of text are classified or labeled. may create an input vector that we must use feature engineering techniques as an input to the ML algorithm. This often means altering the data and making assumptions about the variables in the data that we believe are most pertinent to the predictability of the data. An example is the Naive Bayes and Bag-of-Words representation.\n",
        "\n",
        "To train a model -- for example, training a logistic regression model to determine whether or not a movie review is positive or negative, for example-- we split the labeled data into a training and test sets. First, we will run the algorithm on the training test data, and then evaluate its efficacy. Then, we run the test dataset through the model to evaluate its performance.\n",
        "\n",
        "As we evaluate the performance of the model, we tune \"hyperparameters\". Hyperparameters are inputs to our model that have an influence on the models' performance. They are most often inputs by humans and determined through a series of heuristics and they result in estimates to the model parametters. For example, the percentage of data split between a training and test set is a heuristic -- or rule of thumb-- where we often choose 80% of the labeled data to train our model, and 20% to test it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niCiauS44zRm"
      },
      "source": [
        "#### 3.1.1 Example: ML Approach with NLP - Sentiment Analysis Using Bag-of-Words\n",
        "We often call the Naïve Bayes classifier the bag-of-words approach. That’s because we are essentially throwing in the collection of words into a ‘bag’, selecting a word at random, and then calculating their frequency to use in the Bayesian Inference. Thus, context – the position of words -- is ignored and despite this, it turns out that the Naïve Bayes approach can be accurate and effective at determining whether an email is spam for example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiAq2yoqEFci"
      },
      "source": [
        "###### 3.1.1.1 Load the Dataset and Inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hAUE4tB45S2",
        "outputId": "ebaaf2e5-b873-4fcf-f8a3-ad32f7f48115"
      },
      "source": [
        "#from: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/ml-sklearn-classification.html#data-loading\n",
        "#import libraries\n",
        "import nltk, random\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Load the data from nltk.corpus.moviereviews\n",
        "print(len(movie_reviews.fileids()))\n",
        "print(movie_reviews.categories())\n",
        "print(movie_reviews.words()[:100])\n",
        "print(movie_reviews.fileids()[:10])\n",
        "\n",
        "#Rearrange the corpus data as a list of tuple, where the first element is the word tokens of the documents,\n",
        "#and the second element is the label of the documents (i.e., sentiment labels).\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.seed(123)\n",
        "random.shuffle(documents)\n",
        "\n",
        "#Describe the dataset\n",
        "print('Number of Reviews/Documents: {}'.format(len(documents)))  #Corpus Size (Number of Documents)\n",
        "print('Corpus Size (words): {}'.format(np.sum([len(d) for (d,l) in documents]))) #Corpus Size (Number of Words)\n",
        "print('Sample Text of Doc 1:') #Distribution of the Two Classes\n",
        "print('-'*30)\n",
        "print(' '.join(documents[0][0][:50])) # first 50 words of the first document\n",
        "\n",
        "## Check Sentiment Distribution of the Current Dataset\n",
        "from collections import Counter\n",
        "sentiment_distr = Counter([label for (words, label) in documents])\n",
        "print(sentiment_distr)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "['neg', 'pos']\n",
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n",
            "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
            "Number of Reviews/Documents: 2000\n",
            "Corpus Size (words): 1583820\n",
            "Sample Text of Doc 1:\n",
            "------------------------------\n",
            "most movies seem to release a third movie just so it can be called a trilogy . rocky iii seems to kind of fit in that category , but manages to be slightly unique . the rocky formula of \" rocky loses fight / rocky trains / rocky wins fight\n",
            "Counter({'pos': 1000, 'neg': 1000})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh028orQEBbi"
      },
      "source": [
        "###### 3.1.1.2 Split the data into a training and testing set.\n",
        "\n",
        "Because in most of the ML steps, the feature sets and the labels are often separated as two units, we split our training data into X_train and y_train as the features (X) and labels (y) in training.\n",
        "\n",
        "Likewise, we split our testing data into X_test and y_test as the features (X) and labels (y) in testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmmY9tMPEKei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd6904e-328c-4889-9e39-1eb9e5a62ddd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(documents, test_size = 0.33, random_state=42)\n",
        "## Sentiment Distrubtion for Train and Test\n",
        "print(Counter([label for (words, label) in train]))\n",
        "print(Counter([label for (words, label) in test]))\n",
        "\n",
        "X_train = [' '.join(words) for (words, label) in train]\n",
        "X_test = [' '.join(words) for (words, label) in test]\n",
        "y_train = [label for (words, label) in train]\n",
        "y_test = [label for (words, label) in test]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'neg': 674, 'pos': 666})\n",
            "Counter({'pos': 334, 'neg': 326})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zD53QhgHpLB"
      },
      "source": [
        "##### 3.1.1.3 Text Vectorization\n",
        "In feature-based machine learning, we need to vectorize texts into feature sets (i.e., feature engineering on texts).\n",
        "\n",
        "We use the naive bag-of-words text vectorization. In particular, we use the weighted version of BOW.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux7ShwsdHydx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74e1bb1-51ff-42fa-f23f-0b63b23e52a9"
      },
      "source": [
        "#Note: Always split the data into train and test first before vectorizing the texts.\n",
        "#Otherwise, you would leak information to the training process, which may lead to over-fitting\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "X_train_bow = tfidf_vec.fit_transform(X_train) # fit train\n",
        "X_test_bow = tfidf_vec.transform(X_test) # transform test\n",
        "\n",
        "print(X_train_bow.shape)\n",
        "print(X_test_bow.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1340, 6138)\n",
            "(660, 6138)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzASDHmcH-Mj"
      },
      "source": [
        "##### 3.1.1.4 Model Selection and Cross Validation\n",
        "For our current binary sentiment classifier, we will try a few common classification algorithms:\n",
        "\n",
        "1.   Support Vector Machine\n",
        "2.   Decision Tree\n",
        "3.   Naive Bayes\n",
        "4.   Logistic Regression\n",
        "\n",
        "The common steps include:\n",
        "\n",
        "1.   We fit the model with our training data.\n",
        "2.   We check the model stability, using k-fold cross validation on the training data.\n",
        "3.   We use the fitted model to make prediction.\n",
        "4.   We evaluate the model prediction by comparing the predicted classes and the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcwtL7asIgkJ"
      },
      "source": [
        "###### 3.1.1.5.1 Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw6wI3zoIm9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570d5c0c-60be-49eb-a22f-2fd3ee90b651"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "model_svm = svm.SVC(C=8.0, kernel='linear')\n",
        "model_svm.fit(X_train_bow, y_train)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "model_svm_acc = cross_val_score(estimator=model_svm, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "model_svm_acc\n",
        "\n",
        "model_svm.predict(X_test_bow[:10])\n",
        "print(model_svm.score(X_test_bow, y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8075757575757576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyVytywPIspy"
      },
      "source": [
        "###### 3.1.1.5.2 Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jf2pSbHIxdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774bd377-8995-48b0-b4e4-f7ec2aff3230"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dec = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
        "model_dec.fit(X_train_bow, y_train)\n",
        "\n",
        "model_dec_acc = cross_val_score(estimator=model_dec, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "model_dec_acc\n",
        "\n",
        "model_dec.predict(X_test_bow[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['pos', 'neg', 'neg', 'neg', 'pos', 'pos', 'neg', 'neg', 'neg',\n",
              "       'neg'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXlXXem9I18S"
      },
      "source": [
        "###### 3.1.1.5.3 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsQcANavJNOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176a42bf-2b16-4a84-d518-e0cad59fbfd5"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_gnb = GaussianNB()\n",
        "model_gnb.fit(X_train_bow.toarray(), y_train)\n",
        "\n",
        "model_gnb_acc = cross_val_score(estimator=model_gnb, X=X_train_bow.toarray(), y=y_train, cv=5, n_jobs=-1)\n",
        "model_gnb_acc\n",
        "\n",
        "model_gnb.predict(X_test_bow[:10].toarray())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
              "       'neg'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXV3d8bJTLy"
      },
      "source": [
        "###### 3.1.1.5.3 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuZrQlHUJPTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13b60da-dc06-4c67-f93a-252331f4d967"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lg = LogisticRegression()\n",
        "model_lg.fit(X_train_bow, y_train)\n",
        "\n",
        "model_lg_acc = cross_val_score(estimator=model_lg, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "model_lg_acc\n",
        "\n",
        "model_lg.predict(X_test_bow[:10].toarray())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg',\n",
              "       'pos'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpcXLgo_Jc0B"
      },
      "source": [
        "##### 3.1.1.3 Evaluation\n",
        "\n",
        "To evaluate each model’s performance, there are several common metrics in use:\n",
        "\n",
        "Precision\n",
        "\n",
        "1.   Precision\n",
        "2.   Recall\n",
        "3.   F-score\n",
        "4.   Accuracy\n",
        "5.   Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAWdYNosocvv",
        "outputId": "5c83082d-d45f-4a47-d72f-2cb36a7c65a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VxDTvfVKXbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "ed4e00fa-e63f-40ae-8131-63a9856f36d1"
      },
      "source": [
        "#Mean Accuracy\n",
        "print(model_svm.score(X_test_bow, y_test))\n",
        "print(model_dec.score(X_test_bow, y_test))\n",
        "print(model_gnb.score(X_test_bow.toarray(), y_test))\n",
        "print(model_lg.score(X_test_bow, y_test))\n",
        "\n",
        "# F1\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred = model_svm.predict(X_test_bow)\n",
        "\n",
        "f1_score(y_test, y_pred,\n",
        "         average=None,\n",
        "         labels = movie_reviews.categories())\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(model_svm, X_test_bow, y_test, normalize='all')\n",
        "plot_confusion_matrix(model_lg, X_test_bow.toarray(), y_test, normalize='all')\n",
        "\n",
        "## try a whole new self-created review:)\n",
        "new_review =['This book looks soso like the content but the cover is weird',\n",
        "             'This book looks soso like the content and the cover is weird'\n",
        "            ]\n",
        "new_review_bow = tfidf_vec.transform(new_review)\n",
        "model_svm.predict(new_review_bow)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8075757575757576\n",
            "0.65\n",
            "0.7015151515151515\n",
            "0.7954545454545454\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b6bc63f3ea19>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m          labels = movie_reviews.categories())\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uc7yLN-KfN6"
      },
      "source": [
        "##### 3.1.1.4 Tuning Hyperparameters\n",
        "For each model, we have not optimized it in terms of its hyperparameter setting.\n",
        "\n",
        "Now that SVM seems to perform the best among all, we take this as our base model and further fine-tune its hyperparameter using cross-validation and Grid Search.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPZJnNT_KqI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "a3645cb7-7094-4a34-d1a5-cc7ac8349d80"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'kernel': ('linear', 'rbf'), 'C': (1,4,8,16,32)}\n",
        "\n",
        "svc = svm.SVC()\n",
        "clf = GridSearchCV(svc, parameters, cv=10, n_jobs=-1) ## `-1` run in parallel\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "#We can check the parameters that yield the most optimal results in the Grid Search:\n",
        "\n",
        "print(clf.best_params_)\n",
        "print(clf.score(X_test_bow, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BrokenProcessPool",
          "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute '_passthrough_scorer' on <module 'sklearn.metrics._scorer' from '/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py'>\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-666d8e0d323c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## `-1` run in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0mhere\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mHalvingRandomSearchCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \"\"\"\n\u001b[0;32m--> 874\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m             \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_routing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m             {\n\u001b[0;32m-> 1388\u001b[0;31m             'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n\u001b[0m\u001b[1;32m   1389\u001b[0m                                          mask = [False False False False]...)\n\u001b[1;32m   1390\u001b[0m             'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mvalid_refit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         if (\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid_refit_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Capture the thread-local scikit-learn configuration at the time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqTkIiXy45dG"
      },
      "source": [
        "### 3.2 Introduction to Neural Networks for NLP\n",
        "\n",
        "With the advancement of computational efficiency and resource availability combined with the availability of large amounts of data came the rising importance of Neural Networks and Deep Learning. Especially as it pertains to NLP.\n",
        "\n",
        "*What is Deep Learning?*\n",
        "Deep Learning is a type of machine learning based on artifical neaural networks in which multiple layers of processing are used to extract progressively higher levels of features from data.\n",
        "\n",
        "*What is used for?*\n",
        "Common segments of Deep Learning include NLP tasks, image processing, and time/sequence data analysis like predicting stock market trends or the weather.\n",
        "\n",
        "*How is it different from Machine Learning?*\n",
        "There are several differences (but a lot more in common). Primarily, neural networks enable models to learn non-linear decision boundaries instead of strict linear boundaries. Moreover, Deep Learning notorious does away with feature extraction and engineering.\n",
        "\n",
        "Non-linear decision boundaries compared to classical linear output for Machine Learning\n",
        "![Artificial Intelligence](https://drive.google.com/uc?export=view&id=1cUbV4UZDThbmcKsJKKQGsreEOmkWQSeS)\n",
        "\n",
        "ML vs DL\n",
        "![Artificial Intelligence](https://drive.google.com/uc?export=view&id=1cSP4uxjq-8IL8xRiDN5xRHveTNnPoHp1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXgv2idTSh9X"
      },
      "source": [
        "#### 3.1.1 Types of Neural Networks\n",
        "There are several types of Neural Networks that can be used to achieve different predictive goals. For example, we commonly use Convolutional Neural Networks to process image tasks (or non-sequential tasks) and we use a very of Recurrent Neural Networks to complete sequence-based tasks like time series for stock predictions or translating a sentence from left to right.\n",
        "\n",
        "The following diagram shows the types of Networks that support sequential and non-sequential data.\n",
        "\n",
        "![Neural Networks](https://drive.google.com/uc?export=view&id=12Ixtwys-z3_vv1ema0xyonYOffAWn5p1)\n",
        "\n",
        "##### 2.1.2 Characteristics of the types of NN ([from Chen, 2020](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/dl-neural-network-from-scratch.html))\n",
        "\n",
        "*Multi-Layer Perceptron (Fully Connected Network)*\n",
        "*   Input Layer, one or more hidden layers, and output layer\n",
        "*  A hidden layer consists of neurons (perceptrons) which process certain aspect of the features and send the processed information into the next hidden layer.\n",
        "\n",
        "*Convolutional Neural Network (CNN)*\n",
        "*   Mainly for image and audio processing\n",
        "*   Convolution Layer, Pooling Layer, Fully Connected Layer\n",
        "\n",
        "*Recurrent Neural Network (RNN)*\n",
        "*   fully-connected networks do not remember the steps from previous situations and therefore do not learn to make decisions based on context in training.\n",
        "*  RNN stores the past information and all its decisions are taken from what it has learned from the past.\n",
        "*   RNN is effective in dealing with time-series data (e.g., text, speech).\n",
        "*   Preferred methods in NLP\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTMKd5MhSqML"
      },
      "source": [
        "#### 3.1.2 Characteristics of the Neural Network\n",
        "\n",
        "The following image shows a basic forward propogation Neural Network![NN GIF](https://drive.google.com/uc?export=view&id=1cPN0fK69ncwFD-Idaesvc4LvSLDpHhbO)\n",
        "\n",
        "Generically, a Neural Network will include  (from Chen, 2020):\n",
        "\n",
        "*   **Forward Propagation**: the process of the model taking a series of inputs, manipulating and transforming them, running them through the hidden layers, and producing a predictive output layer.\n",
        "*   **Backward Propagation**: the process of comparing the outputs of the model and then updating the weights in your model to adjust for the observed output compared to the expected output (called loss).\n",
        "*   **Weights**: A vector of weights that are part of the \"hidden layer\". Weights are multiplied by the input layer or previous hidden layer to teach the model which neurons should be activated. Thus, they are an input into the neuron. The also get trained to be more accurate through backpropogation.\n",
        "*   **Neurons**: The component of the Neural Network that is its namesake!. This allow us to model non-linear relationships between input and output data.\n",
        "*   **Activation Functions**:  the activation function of a node determines whether the node would activate the output given the weighted sum of the input values.\n",
        "*   **Nodes to Layers**: neural network can be defined in terms of depths and widths of its layers\n",
        "*   **Layer, Parameters, and Matrix Mutiplication**: Each layer transforms the input values into the output values based on its layer parameters.\n",
        "*   **Hyperparameters**: similar to ML, these are typically human inputs to the model to refine the models predictive efficacy.\n",
        "*   **Loss Function**: If the target ouputs are numeric values, we can evaluate the errors. The loss function (termed cross entropy) represents the function of showing the actual distance of the observed output against the expected output. We can use this information to update our network to be better at predicting in our backpropogation process.\n",
        "*   **Learning Rate and Gradient Descent**: Using the Loss Function, we can now perform the most important step in model training — adjusting the weights (i.e., parameters) of the model. This optimization method to finding a combination of weights that minimize the loss function. The learning rate is a hyperparameter that controls how fast the model learns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-3O5oiq5FBO"
      },
      "source": [
        "#### 3.2.3 Example: Neural Network Approach for NLP\n",
        "\n",
        "Please refer (here) [https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/dl-sentiment-case.html#prepare-data] for an example of NLP using various types of Neural Networks.,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O4qWBwE5PXe"
      },
      "source": [
        "### 3.3 Introduction to Recurrent Neural Networks\n",
        "\n",
        "Recurrent neural network (RNN) \"contains loops, allowing information to be stored within the network. In short, Recurrent Neural Networks use their reasoning from previous experiences to inform the upcoming events.\"\n",
        "\n",
        "A common example of an RNN is machine translation. For example, the *sequence* of the sentence is used to translate from one language to another.\n",
        "\n",
        "\n",
        "See the image below of the RNN Formula:\n",
        "\n",
        "![Neural Networks](https://drive.google.com/uc?export=view&id=12OLUdjs-cDP--rRVU2DziuiWUYKUiruw)\n",
        "\n",
        "See additional the different types of RNNs:![Neural Networks](https://drive.google.com/uc?export=view&id=12MRBEOEukvOzkZt6yvcQJwDwrHSj18dh)\n",
        "\n",
        "Please read the following for a great Illustrated Guide to [Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VbcnhR5Wqe"
      },
      "source": [
        "### 3.4 Exercise: Neural Network for NLP\n",
        "\n",
        "Use the Brown corpus (nltk.corpus.brown) to create a trigram-based neural language model.\n",
        "\n",
        "Please use the language model to generate 50-word text sequences using the seed text “The news”. Provide a few examples from your trained model.\n",
        "\n",
        "A few important notes in data preprocessing:\n",
        "\n",
        "When preparing the input sequences of trigrams for model training, please make sure the trigram does not span across “sentence boundaries”. You can utilize the sentence tokenization annotations provided by the ntlk.corpus.brown.sents().\n",
        "\n",
        "The neural language model will be trained based on all trigrams that fulfill the above criterion in the entire Brown corpus.\n",
        "\n",
        "When you use your trigram-based neural language model to generate sequences, please add randomness to the sampling of the next word. If you always ask the language model to choose the next word of highest predicted probability value, your text would be very repetitive.\n",
        "\n",
        "Please provide your code response in the cell below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import necessary packages and download NLTK data"
      ],
      "metadata": {
        "id": "361fS1TpFfct"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtfFHTn75dSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5bba51-bc5a-40d9-af73-4e467cc4f99f"
      },
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "sentences = brown.sents()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing"
      ],
      "metadata": {
        "id": "SbYK0WpBFmgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generate trigrams, ensuring they do not span across sentence boundaries\n",
        "trigrams = []\n",
        "for sentence in sentences:\n",
        "    sentence = ['<s>'] + sentence + ['</s>']\n",
        "    trigrams.extend([(sentence[i], sentence[i+1], sentence[i+2]) for i in range(len(sentence) - 2)])\n",
        "\n",
        "# 2. Flatten list of words and calculate word frequencies\n",
        "flat_words = [word for sentence in sentences for word in sentence]\n",
        "word_freq = Counter(flat_words)\n",
        "\n",
        "# 3. We need to limit vocabulary size to the top 2000 most common words, otherwise it's not possible to fit the model in this enviroment\n",
        "vocab_size = 2000\n",
        "most_common_words = [word for word, _ in word_freq.most_common(vocab_size)]\n",
        "vocab = set(most_common_words + ['<s>', '</s>'])\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# 4. We now filter trigrams to include only those with words in the limited vocabulary\n",
        "filtered_trigrams = [(w1, w2, w3) for w1, w2, w3 in trigrams if w1 in vocab and w2 in vocab and w3 in vocab]\n",
        "\n",
        "# 5. Prepare data for training\n",
        "X = []\n",
        "y = []\n",
        "for w1, w2, w3 in filtered_trigrams:\n",
        "    X.append([word2idx[w1], word2idx[w2]])\n",
        "    y.append(word2idx[w3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = to_categorical(y, num_classes=len(vocab))"
      ],
      "metadata": {
        "id": "bJEpwMNkFQ3D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define and train the model"
      ],
      "metadata": {
        "id": "Ee_v2-UCFsnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(vocab), output_dim=50, input_length=2)) # Input nodes equal to the number of unique words, each input is 2 words (to predict the 3rd)\n",
        "model.add(LSTM(50, return_sequences=False)) # Using LSTM to avoid vanishing gradients. Only return the last output in the sequence.\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YviTzwoPFU20",
        "outputId": "79362446-1824-4b7b-9717-e4adbf4ea29d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2102/2102 [==============================] - 37s 16ms/step - loss: 5.2391 - accuracy: 0.1532\n",
            "Epoch 2/20\n",
            "2102/2102 [==============================] - 28s 13ms/step - loss: 4.6029 - accuracy: 0.2220\n",
            "Epoch 3/20\n",
            "2102/2102 [==============================] - 16s 7ms/step - loss: 4.4099 - accuracy: 0.2391\n",
            "Epoch 4/20\n",
            "2102/2102 [==============================] - 20s 10ms/step - loss: 4.3111 - accuracy: 0.2452\n",
            "Epoch 5/20\n",
            "2102/2102 [==============================] - 20s 10ms/step - loss: 4.2458 - accuracy: 0.2501\n",
            "Epoch 6/20\n",
            "2102/2102 [==============================] - 16s 7ms/step - loss: 4.1963 - accuracy: 0.2543\n",
            "Epoch 7/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 4.1555 - accuracy: 0.2580\n",
            "Epoch 8/20\n",
            "2102/2102 [==============================] - 16s 8ms/step - loss: 4.1206 - accuracy: 0.2604\n",
            "Epoch 9/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 4.0904 - accuracy: 0.2627\n",
            "Epoch 10/20\n",
            "2102/2102 [==============================] - 16s 7ms/step - loss: 4.0642 - accuracy: 0.2650\n",
            "Epoch 11/20\n",
            "2102/2102 [==============================] - 16s 7ms/step - loss: 4.0406 - accuracy: 0.2667\n",
            "Epoch 12/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 4.0199 - accuracy: 0.2680\n",
            "Epoch 13/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 4.0007 - accuracy: 0.2696\n",
            "Epoch 14/20\n",
            "2102/2102 [==============================] - 16s 7ms/step - loss: 3.9839 - accuracy: 0.2708\n",
            "Epoch 15/20\n",
            "2102/2102 [==============================] - 16s 8ms/step - loss: 3.9681 - accuracy: 0.2717\n",
            "Epoch 16/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 3.9537 - accuracy: 0.2726\n",
            "Epoch 17/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 3.9402 - accuracy: 0.2732\n",
            "Epoch 18/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 3.9282 - accuracy: 0.2743\n",
            "Epoch 19/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 3.9166 - accuracy: 0.2749\n",
            "Epoch 20/20\n",
            "2102/2102 [==============================] - 15s 7ms/step - loss: 3.9058 - accuracy: 0.2758\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78c74d534a60>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create function to generate sentences"
      ],
      "metadata": {
        "id": "Q3uOpuRvFvM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len=2):\n",
        "    text = seed_text.split()\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        token_list = [word2idx[word] for word in text[-max_sequence_len:]]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "        # Predict probabilities for the next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Sample the index of the next word using the predicted probabilities\n",
        "        predicted_idx = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "\n",
        "        # Map index to word\n",
        "        predicted_word = idx2word[predicted_idx]\n",
        "\n",
        "        # Append predicted word to the text\n",
        "        text.append(predicted_word)\n",
        "\n",
        "        # Break if we predict an end-of-sentence token\n",
        "        if predicted_word == '</s>':\n",
        "            break\n",
        "\n",
        "    # Join the generated words into a single string\n",
        "    generated_text = ' '.join(text)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "T0LuddTnFab6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate sentences"
      ],
      "metadata": {
        "id": "sMGDbv2VFzET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 5 sentences using the seed text \"The news\"\n",
        "for i in range(5):\n",
        "    print(generate_text(\"The news\", 50, model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1yleOtZFcHJ",
        "outputId": "94a7bb0e-6789-4245-b8be-d77e1622c393"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The news was a sense from which money pointed him the spirit of the analysis of the cold front in his learned for a moment , with leaving your eye had the married home . </s>\n",
            "The news is to be where 15 letters of God '' , which we can even get away without to such an teachers that the fact that was bad , and Dr. Thomas Act '' ) . </s>\n",
            "The news of the various months from ultimate in our audience , but a little high unless not private unity . </s>\n",
            "The news of mass '' ? ? </s>\n",
            "The news was gone to your agreement . </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-mO2xjtjbd"
      },
      "source": [
        "Examples of the 50-word text sequences created by the language model:\n",
        "\n",
        "```\n",
        "The news that was the first time was that the public interest in the first time he was '' and the in the of the state to the of the world of these theories '' and a few days '' he said that a note of the characteristics of the time of\n",
        "\n",
        "\n",
        "The news of rayburn's commitment well known that mine '' he said '' he said he was in his own life and of the most part of the women have been the of her and mother '' said mrs buck have not been as a result of a group of the and\n",
        "\n",
        "\n",
        "The news that is the basic truth in the next day to relax the emotional stimulation and fear that the author of the western world '' and said it was not a little more than the most of the state of the quarrel obtained a qualification that most of these forces as\n",
        "\n",
        "\n",
        "The news and a little of the time we are never trying to find out what he has a small boy and a series of a new crisis the book was not a tax bill was not at the time of the white house would be to the extent to which he\n",
        "\n",
        "\n",
        "The news of the church must be well to the extent of the most important element of the '' the end of the whole world '' he said he was in the of the '' of the and of the state of the is the of his new ideas that had been\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ruAxznty058"
      },
      "source": [
        "##A. References\n",
        "\n",
        "1.   Chapter 7 – Neural Networks. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021.\n",
        "2.   [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
        "3.   [A hands=on intutive approach to Deep Learning Methods for Text Data - Word2Vec,GloVe and FastText](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
        "4.    [Traditional Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
        "5.    [Word Embeddings](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz)\n",
        "6. [CS 224D: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf)\n",
        "7. [Text Vectorization](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html)\n",
        "8. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
        "9. [TF-IDF](https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n",
        "10. [Applying TF-IDF algorithm in practice](https://plumbr.io/blog/programming/applying-tf-idf-algorithm-in-practice)\n",
        "11. [text2vec](http://text2vec.org/similarity.html)\n",
        "12. [Difference between a parameter and a hyperparameter](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)\n",
        "13. [Sentiment Analysis Using Bag-of-Words](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/ml-sklearn-classification.html)\n",
        "14. [LIME of words: interpreting Recurrent Neural Networks predictions](https://data4thought.com/deep-lime.html)\n",
        "15. [Deepai.org](https://deepai.org/machine-learning-glossary-and-terms/recurrent-neural-network)"
      ]
    }
  ]
}