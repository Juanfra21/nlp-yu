{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanfra21/nlp-yu/blob/main/assn-3/Assignment3_Part_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8n2QqmjC0esQ"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Application Tutorial\n",
        "\n",
        "**Note: This tutorial is provided by Google Research.**\n",
        "\n",
        "\n",
        "In this tutorial, we go over basic operations on word vectors. There are many Natural Language Processing (NLP) libraries in Python, such as [NLTK](https://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/), and [spaCy](https://spacy.io/). All of them have their own strength and focus. NLTK is one of the first comprehensive Python libraries for computational linguistics and has a big community. If you have worked on NLP, you probably have heard of it or used it. Gensim is a popular library for topic modeling. It also provides many functionalities similar to NLTK. It supports word embeddings and you can even train word embeddings using gensim. SpaCy is another popular NLP library and it provides built-in support for word vectors. We will use spaCy in this tutorial.  \\\\\n",
        "<br>\n",
        "You will learn:\n",
        "\n",
        "\n",
        "1.   Popular Python machine learning packages (spaCy, sklearn)\n",
        "2.   Calculating word similarity using Word2Vec model\n",
        "3.   Word analogy analysis\n",
        "4.   Calculating sentence similarity using Word2Vec model\n",
        "5.   Dimension reduction techniques for high-dimensional vectors\n",
        "6.   Visualizing Word2Vec in 2D space\n",
        "7.   Sentiment analysis using logistic regression and Word2Vec\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0fymQg_Q068-"
      },
      "cell_type": "markdown",
      "source": [
        "### Preliminary\n",
        "First, let's install the spaCy Python library and download their model for the English language. We only need to do it once. Then we can import the spaCy library and other useful libraries such as numpy (used for linear algebra and vector operations in Python). We can load our downloaded English model in our environment."
      ]
    },
    {
      "metadata": {
        "id": "7VJCepyC9ZbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5a2f4e-a171-4133-9dbd-db7daaae831e"
      },
      "cell_type": "code",
      "source": [
        "# Only needs to be run once at the top of the notebook\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jzPVan-OAIw8"
      },
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import spacy\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9wiQz4BAuQT"
      },
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')  # load the English model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfpycYEG2T3y"
      },
      "cell_type": "markdown",
      "source": [
        "### Word Similarity\n",
        "By representing words in vectors, we can use linear algebra and vector space models to analyze the relationship between words. One simple task is to calculate the cosine of two word vectors, namely the cosine similarity. This cosine similarity measures the semantic similarity of words. While the value ranges from -1 to 1, it is usually used in the non-negative space [0, 1] where 0 means 0 similarity and 1 means extremely similar or even identical."
      ]
    },
    {
      "metadata": {
        "id": "Qm-5ZI0gHp-w"
      },
      "cell_type": "markdown",
      "source": [
        "In order to calculate the cosine similarity between words, we have to know their vector representations first, which are provided by the Word2Vec model. In the spaCy English model, these vector representations (pretrained using Word2Vec) are already provided. All we need to do is to retrieve these words from the spaCy English model and we will have access to these vector representations. \\\\\n",
        "<br>\n",
        "![cosine_sim](https://engineering.aweber.com/wp-content/uploads/2013/02/4AUbj.png)"
      ]
    },
    {
      "metadata": {
        "id": "A-HL5bHMA3RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971083f0-f592-4639-c211-ff5cd91f2a0c"
      },
      "cell_type": "code",
      "source": [
        "# retrieve words from the English model vocabulary\n",
        "cat = nlp.vocab['cat']\n",
        "dog = nlp.vocab['dog']\n",
        "car = nlp.vocab['car']\n",
        "\n",
        "# print the dimension of word vectors\n",
        "print('vector length:', len(cat.vector))\n",
        "\n",
        "# print the word vector\n",
        "print('cat:', cat.vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector length: 300\n",
            "cat: [ 3.7032e+00  4.1982e+00 -5.0002e+00 -1.1322e+01  3.1702e-02 -1.0255e+00\n",
            " -3.0870e+00 -3.7327e+00  5.3875e-01  3.5679e+00  6.9276e+00  1.5793e+00\n",
            "  5.1188e-01  3.1868e+00  6.1534e+00 -4.8941e+00 -2.9959e-01 -3.6276e+00\n",
            "  2.3825e+00 -1.4402e+00 -4.7577e+00  4.3607e+00 -4.9814e+00 -3.6672e+00\n",
            " -1.8052e+00 -2.1888e+00 -4.2875e+00  5.5712e+00 -5.2875e+00 -1.8346e+00\n",
            " -2.2015e+00 -7.7091e-01 -4.8260e+00  1.2464e+00 -1.7945e+00 -8.1280e+00\n",
            "  1.9994e+00  1.1413e+00  3.8032e+00 -2.8783e+00 -4.2136e-01 -4.4177e+00\n",
            "  7.7456e+00  4.9535e+00  1.7402e+00  1.8275e-01  2.4218e+00 -3.1496e+00\n",
            " -3.8057e-02 -2.9818e+00  8.3396e-01  1.1531e+01  3.5684e+00  2.5970e+00\n",
            " -2.8438e+00  3.2755e+00  4.5674e+00  3.2219e+00  3.4206e+00  1.1200e-01\n",
            "  1.0303e-01 -5.8396e+00  4.6370e-01  2.7750e+00 -5.3713e+00 -5.0247e+00\n",
            " -2.0212e+00  5.8772e-01  1.1569e+00  1.3224e+00  4.3994e+00  2.0444e+00\n",
            "  2.1343e+00 -1.9023e+00  2.1469e+00 -2.9085e+00  4.8429e-01 -3.3544e-01\n",
            "  1.4484e+00 -1.5770e+00 -1.1307e+00  2.8320e+00  6.2041e-01  3.7994e+00\n",
            " -3.1162e-01 -6.9221e+00  7.1342e+00  7.2441e+00 -8.9326e+00 -2.7927e+00\n",
            "  2.6613e-01  6.7547e-01  6.7293e+00 -5.8127e+00  3.1567e+00 -1.0634e+00\n",
            " -1.5733e+00  1.3534e+00  3.9218e-01 -8.7077e+00  3.4229e-02  3.3251e+00\n",
            "  4.6713e+00  1.1865e-02  9.8345e-01 -5.3206e-02 -9.1613e+00  6.0161e+00\n",
            " -2.2223e+00  2.5015e+00 -6.0702e-01 -3.6344e-02  7.1884e+00 -1.4431e+00\n",
            "  2.6156e+00 -1.0148e+00  4.1225e+00 -1.8472e+00  4.6292e+00 -2.6506e+00\n",
            " -1.8937e+00  4.1749e+00 -9.6644e+00 -2.4813e+00 -2.7637e+00 -1.0624e+00\n",
            "  3.5988e+00  4.9833e+00  6.4499e-01  2.5784e-01  9.8727e-01 -4.2485e+00\n",
            "  3.4272e-01 -2.2270e+00 -1.8957e+00  8.0796e-01 -2.0265e+00 -6.1828e+00\n",
            " -2.2378e+00  2.8216e+00 -2.0050e+00 -3.8924e+00 -2.9364e-01 -1.6128e+00\n",
            " -6.7874e-01 -1.9855e+00  1.8221e-01  2.1575e+00  4.9825e-01 -1.7326e+00\n",
            "  4.7886e+00  2.9904e+00  8.3447e-01 -4.7417e+00  2.4697e+00  1.3751e+00\n",
            "  4.5358e+00  6.5386e-01  5.5413e+00  2.3963e+00  1.0031e+00 -8.0664e-01\n",
            " -1.4126e+00  2.8689e+00 -8.7339e+00 -2.7457e+00 -3.1805e-01 -2.4484e-01\n",
            "  3.7117e+00 -1.8636e+00  2.9959e-01  6.5062e-02 -1.5682e+00  1.5876e+00\n",
            "  6.9224e-01 -6.7734e+00  3.1065e+00  2.3973e+00 -3.5138e+00  3.4460e+00\n",
            "  3.4252e+00 -5.1906e+00 -6.9372e-01  1.9435e+00 -1.5669e-01  1.9710e+00\n",
            "  8.7743e-01 -8.3110e+00 -4.0306e-01 -5.0165e+00 -5.6309e-02  4.9249e+00\n",
            " -7.1053e+00 -5.2338e+00  2.3535e+00 -2.5255e+00 -2.7785e+00  5.0149e+00\n",
            " -2.8405e+00 -1.8614e+00  2.8818e-03  1.3281e+00  1.0194e+00  3.5155e+00\n",
            "  2.7971e-01  1.3251e+00  1.4386e+00 -6.1719e-01 -2.6864e+00 -3.9613e+00\n",
            "  4.5749e+00 -1.0939e+00  1.3289e+00 -9.5484e-01 -5.4675e+00  2.1607e+00\n",
            "  5.0715e-01  1.4860e-01 -4.8571e+00 -2.2213e+00 -2.3498e-01 -4.2629e+00\n",
            " -8.7002e-01  3.3796e+00 -4.3989e+00  6.1047e+00  3.7927e+00 -6.0760e+00\n",
            "  3.1840e+00 -8.3104e-01 -5.4015e+00 -6.2916e+00  1.2497e+00  1.8026e+00\n",
            " -3.4535e+00 -2.1652e-01 -1.4958e+00  5.7946e-01  2.2505e+00  2.0868e+00\n",
            "  3.9621e-01  1.6076e+00  4.0635e+00 -3.4088e+00 -1.0590e+00 -3.6376e+00\n",
            "  2.0501e+00  1.4785e+00 -1.8906e+00 -2.6215e-01 -5.1386e+00  3.7029e+00\n",
            " -1.8151e+00 -3.2759e+00 -5.1866e+00  2.5485e-01 -4.5696e+00  1.0147e+01\n",
            " -3.0195e+00 -2.4640e+00  7.5459e-01 -5.6395e+00 -5.4095e+00 -2.4363e+00\n",
            " -4.3922e-01 -4.0911e+00 -3.5194e+00  1.8031e+00 -1.3644e-01  6.7990e+00\n",
            "  5.8461e+00  5.3452e-01  1.1042e+00  3.5698e+00  4.4668e+00 -2.4537e+00\n",
            " -2.1832e+00  1.5293e+00 -1.9414e+00 -8.8675e-02 -1.1825e+00 -3.9996e+00\n",
            "  2.8077e+00 -1.8000e+00  4.2545e+00 -1.3813e+00 -2.2921e+00  3.7889e+00\n",
            " -1.5837e+00 -7.2078e-01  4.7743e+00 -3.0923e+00  8.4709e+00  3.0132e-01\n",
            " -5.6173e+00 -5.4610e-01 -4.8459e+00  6.0303e+00 -6.9664e+00  3.1445e+00]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CmpDwypeHSIy"
      },
      "cell_type": "markdown",
      "source": [
        "Try to retrieve some other words and check if they have the same dimension."
      ]
    },
    {
      "metadata": {
        "id": "NO5S4yX9hgZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086e24ca-9a83-46ed-a5b8-a3d0aa3e55c5"
      },
      "cell_type": "code",
      "source": [
        "# try your own words and check if they have the same dimension of the cat vector\n",
        "############# YOUR CODE HERE ################\n",
        "rain_vector = nlp.vocab['rain'].vector\n",
        "cloud_vector = nlp.vocab['cloud'].vector\n",
        "\n",
        "print(len(rain_vector))\n",
        "print(len(cloud_vector))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-g5mO0AfIT94"
      },
      "cell_type": "markdown",
      "source": [
        "After retrieving the words and their vector representations, we can use the built-in similarity function (which implements cosine similarity) to calculate word similarity based on these vectors. Is 'cat' more similar to 'dog' than 'car'? Can you find some properties of cosine similarity?"
      ]
    },
    {
      "metadata": {
        "id": "lpxnslv1DB7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97905dd8-b054-42e0-c0c1-a9a61b74c203"
      },
      "cell_type": "code",
      "source": [
        "# you can calculate the similarity between words using\n",
        "# the built-in 'similarity' function\n",
        "\n",
        "cat = nlp.vocab['cat']\n",
        "lion = nlp.vocab['lion']\n",
        "tiger = nlp.vocab['tiger']\n",
        "\n",
        "print('The similarity between cat and cat:', cat.similarity(cat))\n",
        "print('The similarity between cat and lion:', cat.similarity(lion))\n",
        "print('The similarity between lion and cat:', lion.similarity(cat))\n",
        "print('The similarity between cat and tiger:', cat.similarity(tiger))\n",
        "print('The similarity between lion and tiger:', lion.similarity(tiger))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between cat and cat: 1.0\n",
            "The similarity between cat and lion: 0.3854507803916931\n",
            "The similarity between lion and cat: 0.3854507803916931\n",
            "The similarity between cat and tiger: 0.5670855045318604\n",
            "The similarity between lion and tiger: 0.713609516620636\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pe1G8CvuIxIP"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try some other words. Also, try to calculate the cosine similarity between 'hotel' and 'motel' and the cosine similarity between 'hotel' and 'hospital'. Which one is more similar to 'hotel'? 'motel' or 'hospital'?"
      ]
    },
    {
      "metadata": {
        "id": "QALZbd0Jhjlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f1439c-5ec7-4b78-df46-3d246f344088"
      },
      "cell_type": "code",
      "source": [
        "# calculate the similarity of your own words using the built-in function\n",
        "############# YOUR CODE HERE ################\n",
        "hotel = nlp.vocab['hotel']\n",
        "motel = nlp.vocab['motel']\n",
        "hospital = nlp.vocab['hospital']\n",
        "\n",
        "# what is the similarity between (hotel, motel) and (hotel, hospital)\n",
        "############# YOUR CODE HERE ################\n",
        "print(hotel.similarity(motel))\n",
        "print(hotel.similarity(hospital))\n",
        "\n",
        "if hotel.similarity(motel) > hotel.similarity(hospital):\n",
        "  print('motel is more similar to hotel than hospital')\n",
        "else:\n",
        "  print('hospital is more similar to hotel than motel')\n",
        "#############################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7972080111503601\n",
            "0.4250935912132263\n",
            "motel is more similar to hotel than hospital\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7jwt8kG5Kuof"
      },
      "cell_type": "markdown",
      "source": [
        "Let's compute the cosine similarity manually using its definition below. Then check if the result is the same as the one calculated by the built-in function. \\\\\n",
        "<br>\n",
        "$cosine\\_similarity(A, B) = \\frac{A \\cdot B}{\\left \\| A \\right \\|\\left \\| B \\right \\|}$"
      ]
    },
    {
      "metadata": {
        "id": "_3ZzlyeLhm3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fc4e78-2252-486a-f057-a01e8c59c3f0"
      },
      "cell_type": "code",
      "source": [
        "# try to calculate cosine similarity manually\n",
        "'''\n",
        "cosine of V1 and V2 = dot product of V1 and V2 / product of V1 norm and V2 norm\n",
        "To get the vector representation of a word, use .vector, e.g. car.vector\n",
        "To calculate the dot product of two vectors V1 and V2, use np.dot(V1, V2)\n",
        "To get the norm of a word vector, use .vector_norm, e.g. car.vector_norm,\n",
        "alternatively you can use np.linalg.norm(V1) to calculate the norm of V1\n",
        "'''\n",
        "############# YOUR CODE HERE ################\n",
        "cosine_dog_car = np.dot(dog.vector, car.vector)/(dog.vector_norm*car.vector_norm)\n",
        "\n",
        "print('The similarity between dog and car calculated manually:', cosine_dog_car)\n",
        "#############################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between dog and car calculated manually: 0.32500255\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Ma4PgQeaMZfM"
      },
      "cell_type": "markdown",
      "source": [
        "Now we know how to compare the similarity of two words using pretrained Word2Vec model. We can also use it to help us find semantically similar words, that is given a word retrieve similar words from the vocabulary. \\\\\n",
        "<br>\n",
        "The Python spaCy library hasn't provided such a function to do precisely this yet. We could use other NLP and machine learning libraries, such as [gensim](https://radimrehurek.com/gensim/), to do this with a simple function call. But the implementation is not hard, so let's give it a try! In our customized function, we first find all the words in our vocabulary (that has vector representations). Then we calculate the cosine similarity between our query word and each word in the vocabulary. We sort the similarity score in descending order. Finally, we retrieve the top n most similar words."
      ]
    },
    {
      "metadata": {
        "id": "j6dRHeE6KBq5"
      },
      "cell_type": "code",
      "source": [
        "# function to find similar words\n",
        "def most_similar(word, topn=10):\n",
        "    allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != word.lower_]  # get all words in the vocabulary\n",
        "    by_similarity = sorted(allwords, key=lambda w: word.similarity(w), reverse=True)  # sort words by similarity in descending order\n",
        "    return by_similarity[:topn]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCemJQbROBpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0426e830-e26d-4b60-8582-2aeffbe081e4"
      },
      "cell_type": "code",
      "source": [
        "# find similar words\n",
        "cat_similar = [w.text for w in most_similar(dog)]\n",
        "print('Similar words to cat: ', cat_similar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to cat:  ['cat', 'tiger', 'you', 'car', 'somethin’', 'lion', 'lovin’', \"'cause\", 'lovin', 'she']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CYclgBII2gc7"
      },
      "cell_type": "markdown",
      "source": [
        "### Word Analogy\n",
        "One interesting finding for the Word2Vec model is that it embeds some analogical relationships between words. \\\\\n",
        "<br>\n",
        "*Man is to Woman as King is to Queen* \\\\\n",
        "Man - Woman = King - Queen \\\\\n",
        "<br>\n",
        "*Paris is to France as Madrid is to Spain* \\\\\n",
        "Paris - France = Madrid - Spain \\\\\n",
        "<br>\n",
        "These relationships can be reconstructed using word embeddings. \\\\\n",
        "<br>\n",
        "![analogy](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)"
      ]
    },
    {
      "metadata": {
        "id": "v6Xo9Prcfpoy"
      },
      "cell_type": "code",
      "source": [
        "# word analogy example\n",
        "# king is to man as what is to woman?\n",
        "king = nlp.vocab['king']\n",
        "man = nlp.vocab['man']\n",
        "woman = nlp.vocab['woman']\n",
        "\n",
        "# resulting vector\n",
        "result = king.vector - man.vector + woman.vector\n",
        "\n",
        "# function to compute cosine similarity\n",
        "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vg_R5y2MyWFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd679d22-d01d-4660-8b35-a37b990c2e64"
      },
      "cell_type": "code",
      "source": [
        "# what word does the 'result' vector closely correspond to?\n",
        "\n",
        "# we can first check if the 'result' vector is similar to the 'queen' vector\n",
        "############# YOUR CODE HERE ################\n",
        "queen = nlp.vocab['queen']\n",
        "print('Similarity between queen and result:', cosine(result, queen.vector))\n",
        "#############################################\n",
        "\n",
        "# find all words in our vocabulary (nlp.vocab),\n",
        "# make sure to just retrieve lower case words\n",
        "# and words that actually have vectors (.has_vector)\n",
        "# and filter out 'king', 'man', and 'woman'\n",
        "############# YOUR CODE HERE ################\n",
        "allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != 'king' and w.lower_ != 'man' and w.lower_ != 'woman']\n",
        "#############################################\n",
        "\n",
        "# calculate the cosine similarity between the 'result' vector\n",
        "# and all word vectors in our vocabulary\n",
        "# sort by similarity and print out the most similar one\n",
        "############# YOUR CODE HERE ################\n",
        "candidates = sorted(allwords, key=lambda w: cosine(result, w.vector), reverse=True)\n",
        "print([c.text for c in candidates[:5]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between queen and result: 0.6178014\n",
            "['queen', 'and', 'that', 'r.', 'where']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dHDhFaO2VaFJ"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try: \\\\\n",
        "Paris - France = Madrid - Spain"
      ]
    },
    {
      "metadata": {
        "id": "unaXunKv0Y0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ad3cea-fe5c-4707-aff6-c3a88e4280d6"
      },
      "cell_type": "code",
      "source": [
        "# another example\n",
        "# Paris is to France as Madrid is to what?\n",
        "############# YOUR CODE HERE ################\n",
        "Paris = nlp.vocab['Paris']\n",
        "France = nlp.vocab['France']\n",
        "Madrid = nlp.vocab['Madrid']\n",
        "\n",
        "maybe_Spain = France.vector - Paris.vector + Madrid.vector\n",
        "\n",
        "allwords = [w for w in nlp.vocab if w.has_vector and w.lower_ != 'paris' and w.lower_ != 'madrid' and w.lower_ != 'france']\n",
        "candidates = sorted(allwords, key=lambda w: cosine(maybe_Spain, w.vector), reverse=True)\n",
        "print([c.text for c in candidates[:5]])\n",
        "#############################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'king', 'Colo', 'Calif', 'were']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nFPYDPuX6thm"
      },
      "cell_type": "markdown",
      "source": [
        "### Sentence/Document Level Similarity\n",
        "Using word embeddings, we can also calculate similarity between sentences and documents. More advanced models such as Doc2Vec or neural networks can be used, but in this tutorial we will continue to use Word2Vec model to calculate document similarity. Since sentences and documents are composed of words, one easy way to obtain vector representations for sentences/documents is to calculate the average vectors of words. \\\\\n",
        "<br>\n",
        "Let's try to calculate the similarity among these three sentences:\n",
        "\n",
        "\n",
        "1.   Cats are beautiful animals.\n",
        "2.   Some gorgeous creatures are felines.\n",
        "3.   Dolphins are swimming mammals.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OiS9WQyG8zqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380db983-8595-4eda-cba8-b933c8033f53"
      },
      "cell_type": "code",
      "source": [
        "# Word2Vec model does not provide vector representations for sentences\n",
        "# or documents. How is the similarity between sentences computed?\n",
        "# Since sentences are composed of words, an easy way to obtain the vector\n",
        "# representations of sentences is by averaging the vectors of each word in\n",
        "# the sentence.\n",
        "############# YOUR CODE HERE ################\n",
        "s1 = (nlp.vocab['Cats'].vector + nlp.vocab['are'].vector + nlp.vocab['beautiful'].vector + \\\n",
        "    nlp.vocab['animals'].vector + nlp.vocab['.'].vector)/5\n",
        "s2 = (nlp.vocab['Some'].vector + nlp.vocab['gorgeous'].vector + nlp.vocab['creatures'].vector + \\\n",
        "    nlp.vocab['are'].vector + nlp.vocab['felines'].vector + nlp.vocab['.'].vector)/6\n",
        "s3 = (nlp.vocab['Dolphins'].vector + nlp.vocab['are'].vector + nlp.vocab['swimming'].vector + \\\n",
        "    nlp.vocab['mammals'].vector + nlp.vocab['.'].vector)/5\n",
        "\n",
        "print(cosine(s1, s2))\n",
        "\n",
        "print(cosine(s1, s3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.90675163\n",
            "0.9037428\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w_P6Ep9F67Lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e430ffc-539c-4ed7-e5b1-6b877524b7d8"
      },
      "cell_type": "code",
      "source": [
        "# spaCy also supports similarity calculation between sentences and documents\n",
        "target = nlp(\"Cats are beautiful animals.\")  # text about cats\n",
        "\n",
        "doc1 = nlp(\"Some gorgeous creatures are felines.\")  # text about cats\n",
        "doc2 = nlp(\"Dolphins are swimming mammals.\")  # text about dolphins\n",
        "\n",
        "print('Similarity between target and doc1:', target.similarity(doc1))\n",
        "print('Similarity between target and doc1:', target.similarity(doc2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between target and doc1: 0.9067517259890845\n",
            "Similarity between target and doc1: 0.9037427153904276\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YYs9_tHSFBhd"
      },
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings Visualization\n",
        "Since the word vectors we use have 300 dimensions, we cannot visualize them. One natural way is to apply dimension reduction first and then visualize them. We use a popular dimension reduction technique called [t-SNE](https://lvdmaaten.github.io/tsne/) (you can also use PCA) to reduce the word vectors to 2D and then plot the words in our word analogy example to see if we can find some pattern visually. \\\\\n",
        "<br>\n",
        "An interactive visualization of word embeddings can be found here: \\\\\n",
        "[https://projector.tensorflow.org/](https://projector.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use t-SNE to do dimension reduction, from 300d to 2d\n",
        "tsne_model = TSNE(n_components=2, perplexity=2)  # Set perplexity less than n_samples (4)\n",
        "\n",
        "# get transformed vectors\n",
        "data = np.array([king.vector, man.vector, queen.vector, woman.vector])\n",
        "data_2d = tsne_model.fit_transform(data)\n",
        "\n",
        "labels = ['king', 'man', 'queen', 'woman']\n",
        "\n",
        "# plot the 2d vectors and show their labels\n",
        "plt.scatter(data_2d[:, 0], data_2d[:, 1], s=100)\n",
        "for i, txt in enumerate(labels):\n",
        "    plt.annotate(txt, (data_2d[i,0], data_2d[i,1]), xytext=(2, 3), textcoords='offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "_TNkLlPtk9_W",
        "outputId": "5ed10b9a-64c4-4dcf-abec-1603ef7da5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "XXzQ0M-OW0h0"
      },
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis\n",
        "The major reason for coming up with word embedding models is that we want to use these embeddings which encode the word semantics to help us tackle problems related with natural language. \\\\\n",
        "<br>\n",
        "One such task is sentiment analysis. By analyzing the sentiment of texts, we want to understand whether a given sentence/document is positive or negative. For example, 'the weather is so nice today' has a positive sentiment whereas 'he is bored by the movie' has a negative sentiment. \\\\\n",
        "<br>\n",
        "In this tutorial, we want to use the word embeddings combined with a simple machine learning model ([logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) to do sentiment analysis. Logistic regression is a linear classification model and in our case we want to classify whether a given sentence is positive or negative. So it's a binary classification. \\\\\n",
        "<br>\n",
        "![logistic](https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression_files/logistic_regression_schematic.png)\n",
        "<br>\n",
        "Our training data contains 2,748 from Yelp reviews, IMDB movie reviews, and Amazon reviews. In the dataset, 1 means positive and 0 means negative. The original data can be downloaded from [here](https://www.kaggle.com/rahulin05/sentiment-labelled-sentences-data-set/data), the combined file can be downloaded from [here](https://drive.google.com/file/d/1knrjvDNkiXtviXBoLm5OJY45_kcCmDxe/view?usp=sharing)."
      ]
    },
    {
      "metadata": {
        "id": "8LliIw9nW4Fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6f7ef21d-3fc6-4a4a-b772-9c4b7fb53a49"
      },
      "cell_type": "code",
      "source": [
        "# load files into the environment\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d9b3f1f-45bd-46f8-b034-b0fed56a6292\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d9b3f1f-45bd-46f8-b034-b0fed56a6292\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combined_training.txt to combined_training.txt\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qGWb5845wEZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc15fc9-69ce-4532-a371-990b60b912e4"
      },
      "cell_type": "code",
      "source": [
        "# read data\n",
        "data_raw = []\n",
        "with open('combined_training.txt', newline='') as fr:\n",
        "    reader = csv.reader(fr, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        data_raw.append([row[0], int(row[1])])\n",
        "\n",
        "# print the number of data\n",
        "print(len(data_raw))\n",
        "\n",
        "# print the last data item\n",
        "print(data_raw[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2748\n",
            "[\"Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check.\", 0]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8U9GGTwFzTnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01a272b-a02a-48e2-a25e-072ce4f04d5f"
      },
      "cell_type": "code",
      "source": [
        "x_train = np.array([nlp(d[0]).vector for d in data_raw])\n",
        "y_train = np.array([d[1] for d in data_raw])\n",
        "\n",
        "# print the dimension of x\n",
        "print(x_train.shape)\n",
        "\n",
        "# print the dimension of y\n",
        "print(y_train.shape)\n",
        "\n",
        "# double check\n",
        "print(nlp(data_raw[-1][0]).text, y_train[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2748, 300)\n",
            "(2748,)\n",
            "Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check. 0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "VwSJnk0A15LB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "be1f2958-75e0-47e6-e25b-273416a82800"
      },
      "cell_type": "code",
      "source": [
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "OTXDcQRq2JHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b1c767-3404-48af-a70c-258aba97c07f"
      },
      "cell_type": "code",
      "source": [
        "# predict using trained model\n",
        "predict = logreg.predict(np.array([nlp('the weather today is pleasant').vector, nlp('the food in this restaurant is beyond my expectation').vector]))\n",
        "print(predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ]
    }
  ]
}